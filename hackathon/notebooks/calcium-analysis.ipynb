{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcium event analysis\n",
    "\n",
    "In this notebook, we will analyse the calcium imaging data that was acquired in the laboratory of Flavie Lavoie-Cardinal.\n",
    "\n",
    "The data consists of a series of images that were acquired at a frame rate of 10 Hz. The raw data is not available into this directory. The entire dataset is available for download at this [link](https://s3.valeria.science/flclab-calcium/index.html). A subset of example videos can also be download from this [link](\"https://s3.valeria.science/flclab-calcium/data/subset-testing-dataset.zip\"). \n",
    "\n",
    "The data that is provided within this repository consists of the following:\n",
    "\n",
    "An expert has annotated the data and identified the time points at which calcium events occur in the movies. The expert manually generated the annotation file (csv). The expert then extracted entire calcium traces at each (x, y) location in the movie and stored them in a `numpy` file. \n",
    "\n",
    "The data is stored into two folders:\n",
    "- `data/annotations` contains the csv files\n",
    "- `data/traces` contains the calcium traces for each detected events\n",
    "\n",
    "The goal of this notebook is to analyse the data and to identify the calcium events automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os \n",
    "import glob\n",
    "import pandas\n",
    "import random\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PATH = os.path.join(\"..\", \"data\")\n",
    "FREQ = 10 # Hz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Implement a function named `load_traces` that loads the data from the given files. The function should take as input the path to the traces file and the path to the annotation file and return the following:\n",
    "- `traces`: a numpy array of shape `(n, t)` where `n` is the number of traces and `t` is the number of time points\n",
    "- `annotations`: a list of length `n` where `n` is the number of traces. Each element of the list is a list of time points at which the calcium events occur.\n",
    "\n",
    "Verify that the function works by loading the data and plotting a random trace with the corresponding annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_traces():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple traces on a single plot\n",
    "\n",
    "Plot multiple traces on a single plot using matplotlib. We wish to reproduce a plot that will look similar to the plot below. You do not need to add the annotations on the plot.\n",
    "\n",
    "![image](./img/traces.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline correction\n",
    "\n",
    "Implement a function named `baseline_correction` that takes as input a trace and returns the baseline. It is recommended to do a baseline correction since the calcium traces are noisy and the baseline is not always at zero. Artifacts can be introduced in the acquisition process and the baseline can be shifted. For instance, the baseline can be shifted due to the movement of the sample or the movement of the microscope objective or ambient light. Different methods can be used to correct the baseline.\n",
    "\n",
    "In the next exercice, the baseline correction should be done by implementing a polynomial fit of the trace. The degree of the polynomial fit should be a parameter of the function. The function should return the baseline.\n",
    "\n",
    "Verify your implementation by plotting the trace and the baseline on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_correction():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercice, we will use the already implemented baseline correction method to correct the baseline of all the traces. The method consists in the asymetrically reweighted penalized least squares smoothing (arPLS).\n",
    "\n",
    "The arPLS method has two input parameters:\n",
    "- `lam`: the regularization parameter\n",
    "- `ratio` : the exit condition\n",
    "\n",
    "Can you visualize the baseline corrected traces? What is the impact of the parameters on the baseline correction? What should be the optimal values for these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(y, lam=1e3, ratio=1e-6):\n",
    "    \"\"\"\n",
    "    Baseline estimation using asymmetrically reweighted penalized least squares smoothing (arPLS) algorithm.\n",
    "\n",
    "    :param y: input signal\n",
    "    :param lam: regularization parameter\n",
    "    :param ratio: exit condition\n",
    "\n",
    "    :return: baseline\n",
    "\n",
    "    :references:\n",
    "    [1] Baek, S. J., Park, A., Ahn, Y. J., & Choo, J. (2015). Baseline correction using \n",
    "        asymmetrically reweighted penalized least squares smoothing. Analyst, 140(1), 250-257.\n",
    "    \"\"\"\n",
    "    lam=1e5\n",
    "    N = len(y)\n",
    "    D = scipy.sparse.csc_matrix(numpy.diff(numpy.eye(N), 2))\n",
    "    w = numpy.ones(N)\n",
    "    MAX_ITER = 100\n",
    "\n",
    "    for _ in range(MAX_ITER):\n",
    "        W = scipy.sparse.spdiags(w, 0, N, N)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = scipy.sparse.linalg.spsolve(Z, w * y)\n",
    "        d = y - z\n",
    "        # make d- and get w^t with m and s\n",
    "        dn = d[d < 0]\n",
    "        m = numpy.mean(dn)\n",
    "        s = numpy.std(dn)\n",
    "        wt = 1.0 / (1 + numpy.exp(2 * (d - (2 * s - m)) / s))\n",
    "        # check exit condition and backup\n",
    "        if numpy.linalg.norm(w - wt) / numpy.linalg.norm(w) < ratio:\n",
    "            break\n",
    "        w = wt\n",
    "\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeltaF/F0 calculation\n",
    "\n",
    "The variation of the fluorescence signal is often calculated as the relative change in fluorescence intensity compared to the baseline fluorescence intensity. This is often referred to as the deltaF/F0 calculation. Implement a function named `deltaF` that takes as input a trace, calculates the baseline using the `baseline` method implemented above and returns the deltaF/F0 trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltaF():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal preprocessing - Filters\n",
    "\n",
    "One of the most important steps in the analysis of calcium imaging data is the preprocessing of the signal. The signal is often noisy and the calcium events are not always clearly visible. The signal can be preprocessed using different filters. The filters can be used to remove the noise from the signal, to smooth the signal or to enhance the signal.\n",
    "\n",
    "In this exercise, we will implement a smoothing function to filter out the noise from the data. The following filters can be used:\n",
    "- Median filter\n",
    "- Mean filter\n",
    "- Gaussian filter\n",
    "- Savitzky-Golay filter\n",
    "\n",
    "Implement a function named `filter_signal` that will apply the one of the filter to the trace. The function should take as input the signal and the name of filter to apply. The function should return the filtered signal. Optionally, you can add the parameters of the filter as input parameters of the function using `kwargs`.\n",
    "\n",
    "Verify your implementation by applying the filter to the trace and plotting the original trace and the filtered trace on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal preprocessing - Averaging\n",
    "\n",
    "Another method that can be used to improve the signal-to-noise ratio is to average the signal over multiple trials or similar ROIs. The reason why averaging can improve the signal-to-noise ratio is because the noise is random and the signal is constant. The noise will cancel out when averaging the signal.\n",
    "\n",
    "For more information, follow the discussion at this [link](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Chemometrics_Using_R_(Harvey)/10%3A_Cleaning_Up_Data/10.2%3A_Signal_Averaging).\n",
    "\n",
    "Implement a function named `average_signal` that will average the signal over multiple traces. The function should take as input the traces and corresponding positions and returne the averaged signal. \n",
    "\n",
    "Verify your implementation by averaging the signal over multiple traces and plotting the original traces and the averaged signal on the same plot.\n",
    "\n",
    "*Hint.* The positions of the traces are stored in the annotation file. You can use the positions to average the signal. Consider that two traces can be grouped if they are close to each other. The distance between two traces can be calculated using the Euclidean distance. A distance of 5 pixels can be used as a threshold to group the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_signal():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `average_signal` method from above, implement a function named `load_average_traces` that returns the calcium traces and the annotations for the averaged traces both as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_average_traces():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single event extraction\n",
    "\n",
    "## Averaging\n",
    "\n",
    "The calcium events are often extracted from the signal by averaging the signal over multiple events. This allows to measure the average shape of the calcium event. The average shape of the calcium event can be used to detect the calcium events in the signal.\n",
    "\n",
    "In this exercise, we will extract the calcium events from the signal and plot them on a single plot. The calcium events can be extracted by taking the signal around the time points of the calcium events (+/- 1s). The signal can be averaged over multiple events to obtain the average shape of the calcium event.\n",
    "\n",
    "Plot all calcium events from the dataset on a single plot using the provided annotations. Store all the calcium events in a list called `events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `events` list, plot the average shape of the calcium event and the standard deviation of the calcium event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Once the average shape of the calcium event is obtained, one can try to model the calcium event using quantitative parameters. In your opinion, what are the best parameters to model the calcium event?\n",
    "\n",
    "From the averaged calcium event can you calculate the following parameters:\n",
    "- Amplitude (-)\n",
    "- Rise time (s)\n",
    "- Decay time (s)\n",
    "\n",
    "We will define the amplitude as the maximum value of the calcium event. We will define the rise time as the time it takes for the calcium event to go from 10% to 90%. The decay time is defined as the time it takes for the calcium event to decay from 90% to 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decay time constant can be calculated using the exponential decay model. The exponential decay model is defined as:\n",
    "$$\n",
    "y(t) = A \\exp(- \\lambda t) + C,\n",
    "$$\n",
    "where $A$ is the amplitude, $\\lambda$ is the decay time constant and $C$ is the baseline.\n",
    "\n",
    "Implement a curve fitting method to fit the averaged calcium event using the exponential decay model. Calculate the decay time constant of the averaged event using the exponential decay model.\n",
    "\n",
    "Verify your implementation by plotting the averaged calcium event and the fitted model on the same plot.\n",
    "\n",
    "*Hint.* You can use the `scipy.optimize.curve_fit` method to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic event detection\n",
    "\n",
    "The calcium events can be detected by thresholding the signal. In other words, an event will be considered when the signal is above a certain value. This threshold can be set to a fixed value for all calcium traces or can be set to a dynamic value. In the next exercise, we will try to detect the signals using both methods.\n",
    "\n",
    "## Using a fixed threshold\n",
    "\n",
    "What should be the threshold to detect all events that were detected by the expert?\n",
    "\n",
    "Verify the threshold by plotting the calcium trace, the detected events, and the calculated threshold on the same plot.\n",
    "\n",
    "*Hint.* Iterate over all calcium traces and calculate the signal that would detect all events in the trace.\n",
    "\n",
    "*Hint.* The `scipy.signal.find_peaks` method can be used to detect the peaks in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an adaptive threshold\n",
    "\n",
    "In practice, events will not always be provided by the expert. Hence, it is important to detect the events automatically. As shown above using a fixed threshold can lead to errors in the detection of events. Typically, the threshold can be set to a dynamic value based on the noise level in the signal. The threshold can be set to a multiple of the noise level in the signal. This allows to adapt the detection threshold to the noise level in the signal.\n",
    "\n",
    "What should be the threshold to detect all (or most) events based on the local noise?\n",
    "\n",
    "Calculate the threshold for each trace based on the noise level in the trace. We will define the noise level as the standard deviation of the signal. The threshold can be set to a multiple of the noise level. Verify the threshold by plotting the calcium trace, the detected events, and the calculated threshold on the same plot.\n",
    "\n",
    "Can you improve the threshold calculation by using a global noise level for each calcium traces?\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "Implement a sliding window method to calculate the noise level in the signal. The noise level can be calculated as the standard deviation of the signal in the window.\n",
    "\n",
    "Are all events real events? Can you filter out the false events using prior information about the shape of the events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event detection efficiency\n",
    "\n",
    "Even if the calcium events are detected automatically, it is important to evaluate the efficiency of the threshold that was used to detect the events. The efficiency of the threshold can be calculated by comparing the detected events with the expert annotations. In this section we will calculate the efficiency of the detection algorithm that we have implemented above.\n",
    "\n",
    "The first step is to calculate wheter an event was detected or not. An event is considered detected if the detected event is within a certain time window of the expert annotation. The time window can be set to 1s.\n",
    "\n",
    "From the association it is possible to calculate the following metrics:\n",
    "1. Number of true positives\n",
    "1. Number of false positives\n",
    "1. Number of true negatives\n",
    "1. Accuracy\n",
    "1. Precision\n",
    "1. Recall\n",
    "\n",
    "*Hint.* Have a look at the linear sum assignment problem and how it can be used to calculate the efficiency of the threshold. See this [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods for event detection\n",
    "\n",
    "There exists many methods for event detection in calcium imaging data. Methods like SICT, Suite2p, CaImAn, etc. have been developed for event detection. The methods can incorporate a physics-based model of the calcium imaging data to improve the event detection or can also rely on machine learning methods to detect the events. Here are some of the methods that can be used for event detection:\n",
    "\n",
    "1. Giovannucci, A., Friedrich, J., Gunn, P., Kalfon, J., Brown, B. L., Koay, S. A., Taxidis, J., Najafi, F., Gauthier, J. L., Zhou, P., Khakh, B. S., Tank, D. W., Chklovskii, D. B., & Pnevmatikakis, E. A. (2019). CaImAn an open source tool for scalable calcium imaging data analysis. eLife, 8, e38173. https://doi.org/10.7554/eLife.38173\n",
    "1. Mancini, R., van der Bijl, T., Bourgeois-Jaarsma, Q., Lasabuda, R., & Groffen, A. J. (2018). SICT: Automated detection and supervised inspection of fast Ca2+ transients. Scientific Reports, 8(1), 15523. https://doi.org/10.1038/s41598-018-33847-4\n",
    "1. Pachitariu, M., Stringer, C., Dipoppa, M., Schröder, S., Rossi, L. F., Dalgleish, H., Carandini, M., & Harris, K. D. (2017). Suite2p: Beyond 10,000 neurons with standard two-photon microscopy (p. 061507). bioRxiv. https://doi.org/10.1101/061507\n",
    "1. Pachitariu, M., Stringer, C., & Harris, K. D. (2018). Robustness of Spike Deconvolution for Neuronal Calcium Imaging. Journal of Neuroscience, 38(37), 7976–7985. https://doi.org/10.1523/JNEUROSCI.3339-17.2018\n",
    "1. Pnevmatikakis, E. A., Soudry, D., Gao, Y., Machado, T. A., Merel, J., Pfau, D., Reardon, T., Mu, Y., Lacefield, C., Yang, W., Ahrens, M., Bruno, R., Jessell, T. M., Peterka, D. S., Yuste, R., & Paninski, L. (2016). Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data. Neuron, 89(2), 285–299. https://doi.org/10.1016/j.neuron.2015.11.037\n",
    "1. Theis, L., Berens, P., Froudarakis, E., Reimer, J., Román Rosón, M., Baden, T., Euler, T., Tolias, A. S., & Bethge, M. (2016). Benchmarking Spike Rate Inference in Population Calcium Imaging. Neuron, 90(3), 471–482. https://doi.org/10.1016/j.neuron.2016.04.014\n",
    "1. Vogelstein, J. T., Packer, A. M., Machado, T. A., Sippy, T., Babadi, B., Yuste, R., & Paninski, L. (2010). Fast nonnegative deconvolution for spike train inference from population calcium imaging. Journal of Neurophysiology, 104(6), 3691–3704. https://doi.org/10.1152/jn.01073.2009\n",
    "1. Wang, Y., DelRosso, N. V., Vaidyanathan, T. V., Cahill, M. K., Reitman, M. E., Pittolo, S., Mi, X., Yu, G., & Poskanzer, K. E. (2019). Accurate quantification of astrocyte and neurotransmitter fluorescence dynamics for single-cell and population-level physiology. Nature Neuroscience, 22(11), Article 11. https://doi.org/10.1038/s41593-019-0492-2\n",
    "\n",
    "*Can you implement any of the above methods for event detection?*\n",
    "\n",
    "Verify online if the authors have provided the code for the above methods. If the code is not available, can you implement the method from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike deconvolution using suite2p\n",
    "\n",
    "In this section we will use the suite2p package to perform spike deconvolution. This is a very powerful package that can be used for spike deconvolution. This is reference *3* in the above list.\n",
    "\n",
    "Information about the definition of spike deconvolution [is given in the suite2p documentation](https://suite2p.readthedocs.io/en/latest/FAQ.html#deconvolution-means-what). \n",
    "\n",
    "The deconvolution algorithm is based on the OASIS algorithm. The algorithm is fully described in the following paper: \n",
    "1. Friedrich, J., Zhou, P., & Paninski, L. (2017). Fast online deconvolution of calcium imaging data. PLoS computational biology, 13(3), e1005423.\n",
    "\n",
    "How does the spike deconvolution algorithm performs compared to the thresholding method that we have implemented above?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs suite2p\n",
    "!pip install \"opencv-python-headless<4.3\"\n",
    "!pip install suite2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calcium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
